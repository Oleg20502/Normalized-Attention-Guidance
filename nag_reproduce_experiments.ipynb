{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3512e9",
   "metadata": {},
   "source": [
    "# Normalized Attention Guidance (NAG) — Reproducing Paper Experiments\n",
    "\n",
    "This notebook is a **reproducible experiment driver** for the paper **\"Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models\"** using the authors' official implementation:\n",
    "\n",
    "- Repo: https://github.com/ChenDarYen/Normalized-Attention-Guidance\n",
    "\n",
    "It covers:\n",
    "- Environment setup + installing the repo\n",
    "- Loading NAG pipelines (Flux / SD3.5 / SDXL)\n",
    "- Running **baseline vs NAG** generation on a prompt set (e.g., COCO-5K)\n",
    "- Computing the paper's metrics: **CLIP Score, FID, Patch-FID (PFID), ImageReward**\n",
    "- Saving outputs + aggregating results into a table\n",
    "\n",
    "> Notes  \n",
    "> - Some model IDs are **gated** on Hugging Face (Flux, etc.). You may need an HF token with access.  \n",
    "> - **FID/PFID** require a reference set of real images (COCO val images). If you don't download COCO images, you can still compute CLIP Score & ImageReward.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4037f5",
   "metadata": {},
   "source": [
    "## 0) (Optional) GPU sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14deb4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.0\n",
      "Torch: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "Capability: (8, 0)\n",
      "VRAM (GB): 39.49\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c50a2cba-5534-4efb-a15e-943cebe97bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE CONFIGURATION - Set your device here\n",
    "DEVICE = \"cuda:2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d44795",
   "metadata": {},
   "source": [
    "## 2) Imports + utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48cdb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import hashlib\n",
    "from dataclasses import dataclass, replace\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# NAG pipelines (from the repo we installed)\n",
    "from nag import (\n",
    "    NAGFluxPipeline,\n",
    "    NAGFluxTransformer2DModel,\n",
    "    NAGStableDiffusion3Pipeline,\n",
    "    NAGStableDiffusionXLPipeline,\n",
    ")\n",
    "\n",
    "# SDXL helper deps (optional for some experiments)\n",
    "from diffusers import UNet2DConditionModel, LCMScheduler\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(p: str | Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def save_image(img: Image.Image, path: str | Path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img.save(path)\n",
    "\n",
    "def sha1_of_file(path: str | Path, chunk_size: int = 1 << 20) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c1514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token found in env: True\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "print(\"HF token found in env:\", bool(HF_TOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41764dcb",
   "metadata": {},
   "source": [
    "## 3) Prompt set: COCO-5K loader (captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b0ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A MAN STANDING ON A BED PLAYING A GUITAR',\n",
       " 'A man stands on the platform with his back turned to the train.',\n",
       " 'A bus stopped near the sidewalk at a bus stop.',\n",
       " 'A man carrying a surf board on a sandy beach next to the ocean.',\n",
       " 'A large jetliner flying through a cloudy gray sky.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "COCO_ANN_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "COCO_IMG_URL = \"http://images.cocodataset.org/zips/val2014.zip\"\n",
    "\n",
    "DATA_DIR = ensure_dir(\"data\")\n",
    "COCO_DIR = ensure_dir(DATA_DIR / \"coco2014\")\n",
    "PROMPTS_PATH = COCO_DIR / \"coco5k_prompts.txt\"\n",
    "\n",
    "def _download(url: str, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        print(f\"Already exists: {out_path}\")\n",
    "        return\n",
    "    print(f\"Downloading: {url}\")\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        with open(out_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=1<<20):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "def _unzip(zip_path: Path, dest: Path):\n",
    "    import zipfile\n",
    "    print(f\"Extracting {zip_path} -> {dest}\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(dest)\n",
    "\n",
    "def prepare_coco_annotations():\n",
    "    \"\"\"Downloads COCO 2014 annotations and extracts captions_val2014.json.\"\"\"\n",
    "    zip_path = COCO_DIR / \"annotations_trainval2014.zip\"\n",
    "    _download(COCO_ANN_URL, zip_path)\n",
    "    _unzip(zip_path, COCO_DIR)\n",
    "    ann_path = COCO_DIR / \"annotations\" / \"captions_val2014.json\"\n",
    "    if not ann_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {ann_path}. Extraction may have failed.\")\n",
    "    return ann_path\n",
    "\n",
    "def make_coco5k_prompts(seed: int = 0, n: int = 5000) -> List[str]:\n",
    "    \"\"\"Creates a fixed prompt list from COCO val2014 captions.\"\"\"\n",
    "    ann_path = prepare_coco_annotations()\n",
    "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        d = json.load(f)\n",
    "    captions = [a[\"caption\"].strip() for a in d[\"annotations\"]]\n",
    "    # Paper uses COCO-5K; we sample deterministically.\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(captions)\n",
    "    prompts = captions[:n]\n",
    "    PROMPTS_PATH.write_text(\"\\n\".join(prompts), encoding=\"utf-8\")\n",
    "    print(f\"Wrote {len(prompts)} prompts -> {PROMPTS_PATH}\")\n",
    "    return prompts\n",
    "\n",
    "def load_prompts(path: Path = PROMPTS_PATH) -> List[str]:\n",
    "    if path.exists():\n",
    "        return [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
    "    print(\"Prompt file not found. Creating COCO-5K prompt list now...\")\n",
    "    return make_coco5k_prompts(seed=0, n=5000)\n",
    "\n",
    "# Smoke test: load first 5 prompts\n",
    "prompts = load_prompts()[:5]\n",
    "prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e056f7",
   "metadata": {},
   "source": [
    "## 4) (Optional) Reference images for FID/PFID (COCO val2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This is a large download (~6GB) and requires disk space.\n",
    "# If you don't need FID/PFID, you can skip this.\n",
    "\n",
    "VAL_IMG_DIR = COCO_DIR / \"val2014\"\n",
    "\n",
    "def prepare_coco_val_images():\n",
    "    zip_path = COCO_DIR / \"val2014.zip\"\n",
    "    _download(COCO_IMG_URL, zip_path)\n",
    "    _unzip(zip_path, COCO_DIR)\n",
    "    if not VAL_IMG_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Expected {VAL_IMG_DIR} after extraction.\")\n",
    "    print(\"COCO val images ready:\", VAL_IMG_DIR)\n",
    "\n",
    "# Uncomment to download:\n",
    "# prepare_coco_val_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3057",
   "metadata": {},
   "source": [
    "## 5) Experiment configs (baseline vs NAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f77a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flux_schnell_4step': RunSpec(name='flux_schnell_4step', model_type='flux', model_id='black-forest-labs/FLUX.1-schnell', steps=4, width=1024, height=1024, guidance_scale=0.0, max_sequence_length=256, dtype=torch.bfloat16),\n",
       " 'sd3_5_turbo_8step': RunSpec(name='sd3_5_turbo_8step', model_type='sd3', model_id='stabilityai/stable-diffusion-3.5-large-turbo', steps=8, width=1024, height=1024, guidance_scale=0.0, max_sequence_length=256, dtype=torch.bfloat16),\n",
       " 'sdxl_base_8step': RunSpec(name='sdxl_base_8step', model_type='sdxl', model_id='stabilityai/stable-diffusion-xl-base-1.0', steps=8, width=1024, height=1024, guidance_scale=2.0, max_sequence_length=256, dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Universal negative prompt used in the paper's quantitative comparison.\n",
    "UNIVERSAL_NEG_PROMPT = \"Low resolution, blurry\"\n",
    "\n",
    "# Paper Table-5 hyperparameters (ϕ = nag_scale, τ = nag_tau, α = nag_alpha).\n",
    "# You can adjust if you want to match the repo demos (they often use nag_scale=5 for Flux).\n",
    "PAPER_DEFAULTS = {\n",
    "    # Few-step DiT\n",
    "    \"flux.1-schnell\": dict(nag_scale=4.0, nag_tau=2.5, nag_alpha=0.25, steps=4, guidance_scale=0.0),\n",
    "    \"flux.1-dev\":     dict(nag_scale=4.0, nag_tau=2.5, nag_alpha=0.25, steps=25, guidance_scale=0.0),\n",
    "    # Multi-step UNet (SDXL)\n",
    "    \"sdxl\":           dict(nag_scale=2.0, nag_tau=2.5, nag_alpha=0.5, steps=8, guidance_scale=2.0),\n",
    "    # SD3.5 turbo (few-step-ish)\n",
    "    \"sd3.5-large-turbo\": dict(nag_scale=3.0, nag_tau=2.5, nag_alpha=0.25, steps=8, guidance_scale=0.0),\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class RunSpec:\n",
    "    name: str\n",
    "    model_type: str               # \"flux\", \"sdxl\", \"sd3\"\n",
    "    model_id: str\n",
    "    steps: int\n",
    "    width: int = 1024\n",
    "    height: int = 1024\n",
    "    guidance_scale: float = 0.0   # CFG; many distilled models use 0 or 1\n",
    "    max_sequence_length: int = 256\n",
    "    dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "# Suggested experiment set (edit as needed)\n",
    "RUNS: Dict[str, RunSpec] = {\n",
    "    \"flux_schnell_4step\": RunSpec(\n",
    "        name=\"flux_schnell_4step\",\n",
    "        model_type=\"flux\",\n",
    "        model_id=\"black-forest-labs/FLUX.1-schnell\",\n",
    "        steps=PAPER_DEFAULTS[\"flux.1-schnell\"][\"steps\"],\n",
    "        width=1024, height=1024,\n",
    "        guidance_scale=PAPER_DEFAULTS[\"flux.1-schnell\"][\"guidance_scale\"],\n",
    "        dtype=torch.bfloat16,\n",
    "    ),\n",
    "    \"sd3_5_turbo_8step\": RunSpec(\n",
    "        name=\"sd3_5_turbo_8step\",\n",
    "        model_type=\"sd3\",\n",
    "        model_id=\"stabilityai/stable-diffusion-3.5-large-turbo\",\n",
    "        steps=PAPER_DEFAULTS[\"sd3.5-large-turbo\"][\"steps\"],\n",
    "        width=1024, height=1024,\n",
    "        guidance_scale=PAPER_DEFAULTS[\"sd3.5-large-turbo\"][\"guidance_scale\"],\n",
    "        dtype=torch.bfloat16,\n",
    "    ),\n",
    "    # SDXL example (can be heavy). In the paper they test SDXL on multi-step and also DMD2 4-step.\n",
    "    # Below is a MULTI-STEP SDXL run on the base model; it may not exactly match paper's distilled checkpoints unless you load them.\n",
    "    \"sdxl_base_8step\": RunSpec(\n",
    "        name=\"sdxl_base_8step\",\n",
    "        model_type=\"sdxl\",\n",
    "        model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        steps=PAPER_DEFAULTS[\"sdxl\"][\"steps\"],\n",
    "        width=1024, height=1024,\n",
    "        guidance_scale=PAPER_DEFAULTS[\"sdxl\"][\"guidance_scale\"],\n",
    "        dtype=torch.bfloat16,\n",
    "    ),\n",
    "}\n",
    "\n",
    "RUNS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40f1b1",
   "metadata": {},
   "source": [
    "## 6) Load pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b89daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(run: RunSpec, hf_token: Optional[str] = None, device: str = DEVICE):\n",
    "    \"\"\"Loads a pipeline for the given run spec.\"\"\"\n",
    "    if run.model_type == \"flux\":\n",
    "        transformer = NAGFluxTransformer2DModel.from_pretrained(\n",
    "            run.model_id,\n",
    "            subfolder=\"transformer\",\n",
    "            torch_dtype=run.dtype,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        pipe = NAGFluxPipeline.from_pretrained(\n",
    "            run.model_id,\n",
    "            transformer=transformer,\n",
    "            torch_dtype=run.dtype,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        pipe.to(device)\n",
    "        return pipe\n",
    "\n",
    "    if run.model_type == \"sd3\":\n",
    "        pipe = NAGStableDiffusion3Pipeline.from_pretrained(\n",
    "            run.model_id,\n",
    "            torch_dtype=run.dtype,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        pipe.to(device)\n",
    "        return pipe\n",
    "\n",
    "    if run.model_type == \"sdxl\":\n",
    "        pipe = NAGStableDiffusionXLPipeline.from_pretrained(\n",
    "            run.model_id,\n",
    "            torch_dtype=run.dtype,\n",
    "            variant=\"fp16\" if run.dtype in (torch.float16, torch.bfloat16) else None,\n",
    "        ).to(device)\n",
    "        return pipe\n",
    "\n",
    "    raise ValueError(f\"Unknown model_type: {run.model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c9e68",
   "metadata": {},
   "source": [
    "## 7) Image generation (baseline vs NAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_batch(\n",
    "    pipe,\n",
    "    run: RunSpec,\n",
    "    prompts: List[str],\n",
    "    *,\n",
    "    out_dir: Path,\n",
    "    seed: int,\n",
    "    mode: str,  # \"baseline\" or \"nag\"\n",
    "    nag_negative_prompt: str = UNIVERSAL_NEG_PROMPT,\n",
    "    nag_scale: float = 4.0,\n",
    "    nag_tau: float = 2.5,\n",
    "    nag_alpha: float = 0.25,\n",
    "    batch_size: int = 1,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Generate images and save them. Returns list of saved file paths.\"\"\"\n",
    "    assert mode in {\"baseline\", \"nag\"}\n",
    "    out_dir = ensure_dir(out_dir)\n",
    "\n",
    "    saved = []\n",
    "    set_seed(seed)\n",
    "\n",
    "    gen = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=f\"gen[{mode}]\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "\n",
    "        kwargs = dict(\n",
    "            prompt=batch_prompts,\n",
    "            num_inference_steps=run.steps,\n",
    "            guidance_scale=run.guidance_scale,\n",
    "            generator=gen,\n",
    "        )\n",
    "\n",
    "        # Some models (Flux) expose max_sequence_length\n",
    "        if \"max_sequence_length\" in pipe.__call__.__code__.co_varnames:\n",
    "            kwargs[\"max_sequence_length\"] = run.max_sequence_length\n",
    "\n",
    "        # Image size (some pipelines accept width/height; others might infer)\n",
    "        if \"width\" in pipe.__call__.__code__.co_varnames:\n",
    "            kwargs[\"width\"] = run.width\n",
    "        if \"height\" in pipe.__call__.__code__.co_varnames:\n",
    "            kwargs[\"height\"] = run.height\n",
    "\n",
    "        if mode == \"nag\":\n",
    "            kwargs.update(\n",
    "                nag_negative_prompt=nag_negative_prompt,\n",
    "                nag_scale=nag_scale,\n",
    "                nag_tau=nag_tau,\n",
    "                nag_alpha=nag_alpha,\n",
    "            )\n",
    "\n",
    "        out = pipe(**kwargs)\n",
    "        images = out.images if hasattr(out, \"images\") else out.frames[0]\n",
    "\n",
    "        for j, img in enumerate(images):\n",
    "            idx = i + j\n",
    "            fn = out_dir / f\"{idx:05d}.png\"\n",
    "            save_image(img, fn)\n",
    "            saved.append(fn)\n",
    "\n",
    "    return saved\n",
    "\n",
    "def get_nag_params_for_run(run: RunSpec):\n",
    "    if run.model_type == \"flux\":\n",
    "        d = PAPER_DEFAULTS[\"flux.1-schnell\"] if \"schnell\" in run.model_id.lower() else PAPER_DEFAULTS[\"flux.1-dev\"]\n",
    "    elif run.model_type == \"sd3\":\n",
    "        d = PAPER_DEFAULTS[\"sd3.5-large-turbo\"]\n",
    "    elif run.model_type == \"sdxl\":\n",
    "        d = PAPER_DEFAULTS[\"sdxl\"]\n",
    "    else:\n",
    "        raise ValueError(run.model_type)\n",
    "    return d[\"nag_scale\"], d[\"nag_tau\"], d[\"nag_alpha\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae4dd4",
   "metadata": {},
   "source": [
    "## 8) Quick smoke test (generate a few images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9271836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce27be17be4dc69946fcafc8e408e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Choose a run:\n",
    "run_key = \"sdxl_base_8step\"\n",
    "run = RUNS[run_key]\n",
    "\n",
    "# Small subset for smoke test\n",
    "test_prompts = load_prompts()[:8]\n",
    "seed = 0\n",
    "\n",
    "pipe = load_pipeline(run, hf_token=HF_TOKEN)\n",
    "\n",
    "nag_scale, nag_tau, nag_alpha = get_nag_params_for_run(run)\n",
    "\n",
    "OUT_ROOT = ensure_dir(\"outputs\") / run.name\n",
    "baseline_dir = OUT_ROOT / \"baseline\"\n",
    "nag_dir = OUT_ROOT / \"nag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477081ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a84d8aab844b9d804997cb3d99df6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b5eb689aef4c8c83a15a814fac0b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gen[baseline]:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2c6855555548d4a252b935ce96dab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eb35d02a884155acdfed6464df1652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adf254707d141ba9bbbc5a7a21777e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ffcf710eed4053925b7d132fd37f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad3b7c9f3754e5ebb8091478dfd562b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6297e21eaf04c40bcdd62e62e6ba92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093de99034234e37b8bd6e6a1e5f3d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b403dbe788064cc59367511cca19e12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d629679ddd410ea6ef16407fb2b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gen[nag]:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71eb8e8e11e44f3a3f3ae439219bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662e96350f2c4e43aa855026f8ea2783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a31d7532de4e769deace43383389b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9862976808084c108e42d136a87b29c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f3676c9a014caf8af8541d1d1b4473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023a764f5f89486485efd53ecbaf4b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cf00f3382542b8a02cea1da75611cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea43113be1a14cbdb2aa99a2361a8e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([PosixPath('outputs/sdxl_base_8step/baseline/00000.png'),\n",
       "  PosixPath('outputs/sdxl_base_8step/baseline/00001.png')],\n",
       " [PosixPath('outputs/sdxl_base_8step/nag/00000.png'),\n",
       "  PosixPath('outputs/sdxl_base_8step/nag/00001.png')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_paths = generate_batch(\n",
    "    pipe, run, test_prompts,\n",
    "    out_dir=baseline_dir,\n",
    "    seed=seed,\n",
    "    mode=\"baseline\",\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "nag_paths = generate_batch(\n",
    "    pipe, run, test_prompts,\n",
    "    out_dir=nag_dir,\n",
    "    seed=seed,\n",
    "    mode=\"nag\",\n",
    "    nag_negative_prompt=UNIVERSAL_NEG_PROMPT,\n",
    "    nag_scale=nag_scale,\n",
    "    nag_tau=nag_tau,\n",
    "    nag_alpha=nag_alpha,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "baseline_paths[:2], nag_paths[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e59fce",
   "metadata": {},
   "source": [
    "## 9) Metrics: CLIP Score, FID, Patch-FID (PFID), ImageReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "# --- CLIP Score ---\n",
    "def compute_clip_score(image_paths: List[Path], prompts: List[str], device=DEVICE, clip_model=\"openai/clip-vit-base-patch32\"):\n",
    "    metric = CLIPScore(model_name_or_path=clip_model).to(device)\n",
    "    # torchmetrics expects uint8 images [0, 255] as a tensor Bx3xHxW\n",
    "    to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),  # 0..1 float\n",
    "        transforms.Lambda(lambda x: (x * 255).to(torch.uint8)),\n",
    "    ])\n",
    "\n",
    "    scores = []\n",
    "    for p, txt in tqdm(list(zip(image_paths, prompts)), desc=\"CLIPScore\"):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        img_t = to_tensor(img).unsqueeze(0).to(device)\n",
    "        s = metric(img_t, [txt]).item()\n",
    "        scores.append(s)\n",
    "    return float(np.mean(scores)), scores\n",
    "\n",
    "# --- FID ---\n",
    "def _iter_images_as_uint8_tensor(image_paths: List[Path], size=(299, 299)):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (x * 255).to(torch.uint8)),\n",
    "    ])\n",
    "    for p in image_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        yield tfm(img)\n",
    "\n",
    "def compute_fid(gen_paths: List[Path], real_paths: List[Path], device=DEVICE):\n",
    "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=True).to(device)\n",
    "\n",
    "    for t in tqdm(_iter_images_as_uint8_tensor(real_paths), total=len(real_paths), desc=\"FID(real)\"):\n",
    "        fid.update(t.unsqueeze(0).to(device), real=True)\n",
    "\n",
    "    for t in tqdm(_iter_images_as_uint8_tensor(gen_paths), total=len(gen_paths), desc=\"FID(gen)\"):\n",
    "        fid.update(t.unsqueeze(0).to(device), real=False)\n",
    "\n",
    "    return float(fid.compute().item())\n",
    "\n",
    "# --- Patch-FID (PFID) ---\n",
    "def _extract_patches(img: Image.Image, grid: int = 4) -> List[Image.Image]:\n",
    "    \"\"\"Split image into grid x grid patches.\"\"\"\n",
    "    w, h = img.size\n",
    "    pw, ph = w // grid, h // grid\n",
    "    patches = []\n",
    "    for gy in range(grid):\n",
    "        for gx in range(grid):\n",
    "            left, top = gx * pw, gy * ph\n",
    "            patches.append(img.crop((left, top, left + pw, top + ph)))\n",
    "    return patches\n",
    "\n",
    "def compute_patch_fid(gen_paths: List[Path], real_paths: List[Path], device=DEVICE, grid: int = 4):\n",
    "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=True).to(device)\n",
    "\n",
    "    # update real patches\n",
    "    for p in tqdm(real_paths, desc=\"PFID(real)\", total=len(real_paths)):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        for patch in _extract_patches(img, grid=grid):\n",
    "            t = transforms.ToTensor()(patch)\n",
    "            t = (t * 255).to(torch.uint8)\n",
    "            fid.update(t.unsqueeze(0).to(device), real=True)\n",
    "\n",
    "    # update gen patches\n",
    "    for p in tqdm(gen_paths, desc=\"PFID(gen)\", total=len(gen_paths)):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        for patch in _extract_patches(img, grid=grid):\n",
    "            t = transforms.ToTensor()(patch)\n",
    "            t = (t * 255).to(torch.uint8)\n",
    "            fid.update(t.unsqueeze(0).to(device), real=False)\n",
    "\n",
    "    return float(fid.compute().item())\n",
    "\n",
    "# --- ImageReward ---\n",
    "# def compute_imagereward(image_paths: List[Path], prompts: List[str], device=DEVICE):\n",
    "#     import ImageReward as IR\n",
    "#     model = IR.load(\"ImageReward-v1.0\", device=device)\n",
    "#     scores = []\n",
    "#     for p, txt in tqdm(list(zip(image_paths, prompts)), desc=\"ImageReward\"):\n",
    "#         img = Image.open(p).convert(\"RGB\")\n",
    "#         s = model.score(txt, img)\n",
    "#         scores.append(float(s))\n",
    "#     return float(np.mean(scores)), scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8472ca",
   "metadata": {},
   "source": [
    "## 10) Run evaluation on your generated folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de603fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n(baseline): 8 n(nag): 8 n(prompts): 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dd60bd673743dfb9f6f5f0d2752afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPScore:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15803136cc944db8a183e4f20f8212fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPScore:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>mode</th>\n",
       "      <th>clip_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sdxl_base_8step</td>\n",
       "      <td>baseline</td>\n",
       "      <td>29.505333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sdxl_base_8step</td>\n",
       "      <td>nag</td>\n",
       "      <td>30.482667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run      mode  clip_score\n",
       "0  sdxl_base_8step  baseline   29.505333\n",
       "1  sdxl_base_8step       nag   30.482667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_images_sorted(folder: Path) -> List[Path]:\n",
    "    return sorted([p for p in folder.glob(\"*.png\")])\n",
    "\n",
    "# Use the smoke test output folders by default:\n",
    "gen_baseline = list_images_sorted(baseline_dir)\n",
    "gen_nag = list_images_sorted(nag_dir)\n",
    "\n",
    "eval_prompts = test_prompts  # must match image ordering\n",
    "\n",
    "print(\"n(baseline):\", len(gen_baseline), \"n(nag):\", len(gen_nag), \"n(prompts):\", len(eval_prompts))\n",
    "\n",
    "# ---- CLIP score ----\n",
    "clip_mean_base, _ = compute_clip_score(gen_baseline, eval_prompts)\n",
    "clip_mean_nag, _ = compute_clip_score(gen_nag, eval_prompts)\n",
    "\n",
    "# ---- ImageReward ----\n",
    "# ir_mean_base, _ = compute_imagereward(gen_baseline, eval_prompts)\n",
    "# ir_mean_nag, _ = compute_imagereward(gen_nag, eval_prompts)\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    dict(run=run.name, mode=\"baseline\", clip_score=clip_mean_base,\n",
    "    # image_reward=ir_mean_base\n",
    "),\n",
    "    dict(run=run.name, mode=\"nag\",      clip_score=clip_mean_nag,\n",
    "    # image_reward=ir_mean_nag\n",
    "),\n",
    "])\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed01d44",
   "metadata": {},
   "source": [
    "## 11) (Optional) FID / PFID evaluation (requires COCO real images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8ba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441d613fdfb943ea9c99934a376c773d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FID(real):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0482ef3920824aa4805efee26d9c9001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FID(gen):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d71138c2cc446ca7cc63148d352d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FID(real):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657fba7a439947b3a302eb4d1a196f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FID(gen):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30a7afac954453ca12c9c65518d3082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PFID(real):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a0dc288a5944289dbc7179dd58ec9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PFID(gen):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2464b774b834c99a64aef72dad827ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PFID(real):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b9160ebc0d41008f1a38abe044d2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PFID(gen):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# This section requires COCO validation images downloaded in section 4.\n",
    "# We align on the *same number of images* as generated (e.g., 5k).\n",
    "# NOTE: COCO val2014 has 40k images; paper uses COCO-5K subset. You should define which 5k images you use.\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "def list_coco_real_images(val_dir: Path, n: int) -> List[Path]:\n",
    "    all_imgs = sorted(val_dir.glob(\"*.jpg\"))\n",
    "    if len(all_imgs) < n:\n",
    "        raise ValueError(f\"Not enough COCO images: {len(all_imgs)} < {n}\")\n",
    "    return all_imgs[:n]  # deterministic prefix; change if you want a different fixed subset\n",
    "\n",
    "\n",
    "real_paths = list_coco_real_images(VAL_IMG_DIR, n=len(gen_baseline))\n",
    "\n",
    "fid_base = compute_fid(gen_baseline, real_paths)\n",
    "fid_nag = compute_fid(gen_nag, real_paths)\n",
    "\n",
    "pfid_base = compute_patch_fid(gen_baseline, real_paths, grid=4)\n",
    "pfid_nag = compute_patch_fid(gen_nag, real_paths, grid=4)\n",
    "\n",
    "results_fid = pd.DataFrame([\n",
    "    dict(run=run.name, mode=\"baseline\", fid=fid_base, pfid=pfid_base),\n",
    "    dict(run=run.name, mode=\"nag\",      fid=fid_nag,  pfid=pfid_nag),\n",
    "])\n",
    "results_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862cdef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>mode</th>\n",
       "      <th>fid</th>\n",
       "      <th>pfid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sdxl_base_8step</td>\n",
       "      <td>baseline</td>\n",
       "      <td>353.235657</td>\n",
       "      <td>321.943604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sdxl_base_8step</td>\n",
       "      <td>nag</td>\n",
       "      <td>350.996185</td>\n",
       "      <td>324.413910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run      mode         fid        pfid\n",
       "0  sdxl_base_8step  baseline  353.235657  321.943604\n",
       "1  sdxl_base_8step       nag  350.996185  324.413910"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e610a83",
   "metadata": {},
   "source": [
    "## 12) Full experiment runner (COCO-5K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ed346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(\n",
    "    run: RunSpec,\n",
    "    *,\n",
    "    prompts: List[str],\n",
    "    out_root: Path,\n",
    "    seed: int = 0,\n",
    "    hf_token: Optional[str] = None,\n",
    "    batch_size: int = 1,\n",
    "    nag_negative_prompt: str = UNIVERSAL_NEG_PROMPT,\n",
    "):\n",
    "    out_root = ensure_dir(out_root)\n",
    "    pipe = load_pipeline(run, hf_token=hf_token)\n",
    "\n",
    "    nag_scale, nag_tau, nag_alpha = get_nag_params_for_run(run)\n",
    "\n",
    "    baseline_dir = out_root / \"baseline\"\n",
    "    nag_dir = out_root / \"nag\"\n",
    "\n",
    "    base_paths = generate_batch(\n",
    "        pipe, run, prompts,\n",
    "        out_dir=baseline_dir,\n",
    "        seed=seed,\n",
    "        mode=\"baseline\",\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    nag_paths = generate_batch(\n",
    "        pipe, run, prompts,\n",
    "        out_dir=nag_dir,\n",
    "        seed=seed,\n",
    "        mode=\"nag\",\n",
    "        nag_negative_prompt=nag_negative_prompt,\n",
    "        nag_scale=nag_scale,\n",
    "        nag_tau=nag_tau,\n",
    "        nag_alpha=nag_alpha,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Always compute CLIP + ImageReward (no ref needed)\n",
    "    clip_base, _ = compute_clip_score(base_paths, prompts)\n",
    "    clip_nag, _ = compute_clip_score(nag_paths, prompts)\n",
    "\n",
    "    ir_base, _ = compute_imagereward(base_paths, prompts)\n",
    "    ir_nag, _ = compute_imagereward(nag_paths, prompts)\n",
    "\n",
    "    metrics = pd.DataFrame([\n",
    "        dict(run=run.name, mode=\"baseline\", clip_score=clip_base, image_reward=ir_base),\n",
    "        dict(run=run.name, mode=\"nag\",      clip_score=clip_nag,  image_reward=ir_nag),\n",
    "    ])\n",
    "\n",
    "    # Optionally compute FID/PFID if real images exist\n",
    "    if VAL_IMG_DIR.exists():\n",
    "        real_paths = list_coco_real_images(VAL_IMG_DIR, n=len(prompts))\n",
    "        fid_base = compute_fid(base_paths, real_paths)\n",
    "        fid_nag = compute_fid(nag_paths, real_paths)\n",
    "        pfid_base = compute_patch_fid(base_paths, real_paths, grid=4)\n",
    "        pfid_nag = compute_patch_fid(nag_paths, real_paths, grid=4)\n",
    "        metrics.loc[metrics[\"mode\"]==\"baseline\", \"fid\"] = fid_base\n",
    "        metrics.loc[metrics[\"mode\"]==\"nag\", \"fid\"] = fid_nag\n",
    "        metrics.loc[metrics[\"mode\"]==\"baseline\", \"pfid\"] = pfid_base\n",
    "        metrics.loc[metrics[\"mode\"]==\"nag\", \"pfid\"] = pfid_nag\n",
    "\n",
    "    metrics.to_csv(out_root / \"metrics.csv\", index=False)\n",
    "    return metrics\n",
    "\n",
    "# Example: run on 128 prompts first, then scale up\n",
    "# prompts_5k = load_prompts()[:5000]\n",
    "# metrics = run_experiment(RUNS[\"flux_schnell_4step\"], prompts=prompts_5k, out_root=Path(\"outputs\")/RUNS[\"flux_schnell_4step\"].name, hf_token=HF_TOKEN, batch_size=1)\n",
    "# metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82beb4ba",
   "metadata": {},
   "source": [
    "## 14) Tips for matching the paper more closely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b62fc3",
   "metadata": {},
   "source": [
    "\n",
    "- Use the **same prompt list** and **same real-image subset** when reporting FID/PFID.  \n",
    "  In this notebook we create a deterministic prompt list (seeded shuffle) and select a deterministic prefix of COCO val images — but you should align these with the paper's exact protocol if you have it.\n",
    "\n",
    "- Use the **paper hyperparameters** (ϕ, τ, α) from Table-5:\n",
    "  - Flux: ϕ=4, τ=2.5, α=0.25  \n",
    "  - SDXL: ϕ=2, τ=2.5, α=0.25  \n",
    "  - SD3.5: ϕ=3, τ=2.5, α=0.25  \n",
    "\n",
    "- Many few-step models are designed for **guidance_scale=0 or 1**; CFG can degrade them.  \n",
    "  NAG is intended to restore negative prompting without requiring CFG.\n",
    "\n",
    "- For large-scale runs (COCO-5K), consider:\n",
    "  - `batch_size=1` for safety (VRAM)\n",
    "  - saving intermediate progress\n",
    "  - running on multiple GPUs / nodes with prompt sharding\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01039c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PipelineConfig:\n",
    "    name: str\n",
    "    model_type: str\n",
    "    model_id: str\n",
    "    steps: int # Number of inference steps\n",
    "    guidance_scale: float = 1 # Guidance scale\n",
    "    nag_scale: float = 1 # NAG scale\n",
    "    nag_alpha: float = 0.125 # NAG alpha\n",
    "    nag_tau: float = 2.5 # NAG tau\n",
    "    max_sequence_length: int = 256 # Maximum sequence length\n",
    "    width: int = 1024 # Image width\n",
    "    height: int = 1024 # Image height\n",
    "    seed: int = 42 # Random seed\n",
    "    dtype: torch.dtype = torch.bfloat16 # Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159113f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd3_config = PipelineConfig(\n",
    "    name=\"sd3_5_turbo_8step\",\n",
    "    model_type=\"sd3\",\n",
    "    model_id=\"stabilityai/stable-diffusion-3.5-large-turbo\",\n",
    "    steps=8,\n",
    "    guidance_scale=3,\n",
    "    nag_scale=4,\n",
    "    nag_alpha=0.125,\n",
    "    nag_tau=2.5,\n",
    ")\n",
    "\n",
    "sdxl_config = PipelineConfig(\n",
    "    name=\"sdxl_base_8step\",\n",
    "    model_type=\"sdxl\",\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    steps=8,\n",
    "    guidance_scale=4,\n",
    "    nag_scale=2,\n",
    "    nag_alpha=0.5,\n",
    "    nag_tau=2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25054197",
   "metadata": {},
   "source": [
    "## Exp: changing nag scale $\\Phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b30e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nag_scale_arr = [0, 2.5, 5, 7.5, 10, 15, 20]\n",
    "\n",
    "n_prompts = 100\n",
    "nag_neg_prompt = UNIVERSAL_NEG_PROMPT\n",
    "\n",
    "run = replace(sdxl_config, name=\"sdxl_base_8step\", guidance_scale=0.0)\n",
    "\n",
    "test_prompts = load_prompts()[:n_prompts]\n",
    "\n",
    "OUT_ROOT = ensure_dir(\"outputs\") / run.name\n",
    "exp_dir = OUT_ROOT / \"exp_nag_scale\"\n",
    "\n",
    "real_paths = list_coco_real_images(VAL_IMG_DIR, n=n_prompts)\n",
    "\n",
    "try:\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "pipe = load_pipeline(run, hf_token=HF_TOKEN)\n",
    "\n",
    "results_1 = defaultdict(list)\n",
    "for i, nag_scale in enumerate(nag_scale_arr):\n",
    "    print(f\"Running with NAG scale: {nag_scale}\")\n",
    "    \n",
    "    nag_paths = generate_batch(\n",
    "        pipe, run, test_prompts,\n",
    "        out_dir=exp_dir,\n",
    "        seed=seed,\n",
    "        mode=\"nag\",\n",
    "        nag_negative_prompt=nag_neg_prompt,\n",
    "        nag_scale=nag_scale,\n",
    "        nag_tau=nag_tau,\n",
    "        nag_alpha=nag_alpha,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    gen_images = list_images_sorted(exp_dir)\n",
    "\n",
    "    clip_mean, _ = compute_clip_score(gen_images, test_prompts)\n",
    "\n",
    "    fid_base = compute_fid(gen_images, real_paths)\n",
    "    pfid_base = compute_patch_fid(gen_images, real_paths, grid=4)\n",
    "\n",
    "    results_1[\"nag_scale\"].append(nag_scale)\n",
    "    results_1[\"clip_mean\"].append(clip_mean)\n",
    "    results_1[\"fid_base\"].append(fid_base)\n",
    "    results_1[\"pfid_base\"].append(pfid_base)\n",
    "\n",
    "    print(f\"NAG scale: {nag_scale}, CLIP score: {clip_mean}, FID: {fid_base}, PFID: {pfid_base}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0d2a9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'nag_scale': [0, 2.5, 5, 7.5, 10, 15],\n",
       "             'clip_mean': [24.902287969589235,\n",
       "              25.268795852661132,\n",
       "              25.284650869369507,\n",
       "              25.30036262512207,\n",
       "              25.309369144439696,\n",
       "              25.285631084442137],\n",
       "             'fid_base': [321.5397644042969,\n",
       "              319.117431640625,\n",
       "              318.7151184082031,\n",
       "              318.9698791503906,\n",
       "              318.3301696777344,\n",
       "              318.1114196777344],\n",
       "             'pfid_base': [237.57139587402344,\n",
       "              234.54177856445312,\n",
       "              234.5036163330078,\n",
       "              234.3966522216797,\n",
       "              234.47134399414062,\n",
       "              234.61070251464844]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d24852",
   "metadata": {},
   "source": [
    "## Exp: changing nag scale $\\Phi$ w/o Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nag_scale_arr = [0, 2.5, 5, 7.5, 10, 15, 20]\n",
    "\n",
    "n_prompts = 100\n",
    "nag_neg_prompt = UNIVERSAL_NEG_PROMPT\n",
    "\n",
    "run = replace(sdxl_config, name=\"sdxl_base_8step\", guidance_scale=0.0, nag_alpha=0.0)\n",
    "\n",
    "test_prompts = load_prompts()[:n_prompts]\n",
    "\n",
    "OUT_ROOT = ensure_dir(\"outputs\") / run.name\n",
    "exp_dir = OUT_ROOT / \"exp_nag_scale\"\n",
    "\n",
    "real_paths = list_coco_real_images(VAL_IMG_DIR, n=n_prompts)\n",
    "\n",
    "try:\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "pipe = load_pipeline(run, hf_token=HF_TOKEN)\n",
    "\n",
    "results_2 = defaultdict(list)\n",
    "for i, nag_scale in enumerate(nag_scale_arr):\n",
    "    print(f\"Running with NAG scale: {nag_scale}\")\n",
    "    \n",
    "    nag_paths = generate_batch(\n",
    "        pipe, run, test_prompts,\n",
    "        out_dir=exp_dir,\n",
    "        seed=seed,\n",
    "        mode=\"nag\",\n",
    "        nag_negative_prompt=nag_neg_prompt,\n",
    "        nag_scale=nag_scale,\n",
    "        nag_tau=nag_tau,\n",
    "        nag_alpha=nag_alpha,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    gen_images = list_images_sorted(exp_dir)\n",
    "\n",
    "    clip_mean, _ = compute_clip_score(gen_images, test_prompts)\n",
    "\n",
    "    fid_base = compute_fid(gen_images, real_paths)\n",
    "    pfid_base = compute_patch_fid(gen_images, real_paths, grid=4)\n",
    "\n",
    "    results_2[\"nag_scale\"].append(nag_scale)\n",
    "    results_2[\"clip_mean\"].append(clip_mean)\n",
    "    results_2[\"fid_base\"].append(fid_base)\n",
    "    results_2[\"pfid_base\"].append(pfid_base)\n",
    "\n",
    "    print(f\"NAG scale: {nag_scale}, CLIP score: {clip_mean}, FID: {fid_base}, PFID: {pfid_base}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d1417",
   "metadata": {},
   "source": [
    "## Exp: changing nag scale $\\Phi$ w/o Refine & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nag_scale_arr = [0, 2.5, 5, 7.5, 10, 15, 20]\n",
    "\n",
    "n_prompts = 100\n",
    "nag_neg_prompt = UNIVERSAL_NEG_PROMPT\n",
    "\n",
    "run = replace(sdxl_config, name=\"sdxl_base_8step\",\n",
    "    guidance_scale=0.0, nag_alpha=0.0, nag_tau=1000)\n",
    "\n",
    "test_prompts = load_prompts()[:n_prompts]\n",
    "\n",
    "OUT_ROOT = ensure_dir(\"outputs\") / run.name\n",
    "exp_dir = OUT_ROOT / \"exp_nag_scale\"\n",
    "\n",
    "real_paths = list_coco_real_images(VAL_IMG_DIR, n=n_prompts)\n",
    "\n",
    "try:\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "pipe = load_pipeline(run, hf_token=HF_TOKEN)\n",
    "\n",
    "results_3 = defaultdict(list)\n",
    "for i, nag_scale in enumerate(nag_scale_arr):\n",
    "    print(f\"Running with NAG scale: {nag_scale}\")\n",
    "    \n",
    "    nag_paths = generate_batch(\n",
    "        pipe, run, test_prompts,\n",
    "        out_dir=exp_dir,\n",
    "        seed=seed,\n",
    "        mode=\"nag\",\n",
    "        nag_negative_prompt=nag_neg_prompt,\n",
    "        nag_scale=nag_scale,\n",
    "        nag_tau=nag_tau,\n",
    "        nag_alpha=nag_alpha,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    gen_images = list_images_sorted(exp_dir)\n",
    "\n",
    "    clip_mean, _ = compute_clip_score(gen_images, test_prompts)\n",
    "\n",
    "    fid_base = compute_fid(gen_images, real_paths)\n",
    "    pfid_base = compute_patch_fid(gen_images, real_paths, grid=4)\n",
    "\n",
    "    results_3[\"nag_scale\"].append(nag_scale)\n",
    "    results_3[\"clip_mean\"].append(clip_mean)\n",
    "    results_3[\"fid_base\"].append(fid_base)\n",
    "    results_3[\"pfid_base\"].append(pfid_base)\n",
    "\n",
    "    print(f\"NAG scale: {nag_scale}, CLIP score: {clip_mean}, FID: {fid_base}, PFID: {pfid_base}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
