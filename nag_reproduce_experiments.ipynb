{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Normalized Attention Guidance (NAG) — Reproducing Paper Experiments\n",
        "\n",
        "This notebook is a **reproducible experiment driver** for the paper **\"Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models\"** using the authors' official implementation:\n",
        "\n",
        "- Repo: https://github.com/ChenDarYen/Normalized-Attention-Guidance\n",
        "\n",
        "It covers:\n",
        "- Environment setup + installing the repo\n",
        "- Loading NAG pipelines (Flux / SD3.5 / SDXL)\n",
        "- Running **baseline vs NAG** generation on a prompt set (e.g., COCO-5K)\n",
        "- Computing the paper's metrics: **CLIP Score, FID, Patch-FID (PFID), ImageReward**\n",
        "- Saving outputs + aggregating results into a table\n",
        "\n",
        "> Notes  \n",
        "> - Some model IDs are **gated** on Hugging Face (Flux, etc.). You may need an HF token with access.  \n",
        "> - **FID/PFID** require a reference set of real images (COCO val images). If you don't download COCO images, you can still compute CLIP Score & ImageReward.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) (Optional) GPU sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.0\n",
            "Torch: 2.9.1+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Capability: (8, 0)\n",
            "VRAM (GB): 39.49\n"
          ]
        }
      ],
      "source": [
        "import torch, platform, os, subprocess, textwrap\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
        "    print(\"VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install dependencies + the official NAG repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# If you are running on Colab, you may want:\n",
        "# !pip -q install --upgrade pip\n",
        "\n",
        "# # Core\n",
        "# !pip -q install \"diffusers>=0.30.0\" \"transformers>=4.41.0\" \"accelerate>=0.33.0\" \"safetensors>=0.4.3\" \"huggingface_hub>=0.23.0\"\n",
        "\n",
        "# # Metrics / eval\n",
        "# !pip -q install \"torchmetrics>=1.4.0\" \"pytorch-fid>=0.3.0\" \"ImageReward\" \"opencv-python\" \"scikit-image\" \"pandas\" \"tqdm\" \"matplotlib\"\n",
        "\n",
        "# # Dataset helpers\n",
        "# !pip -q install \"pycocotools\" \"requests\"\n",
        "\n",
        "# Install the authors' implementation\n",
        "# !pip -q install \"git+https://github.com/ChenDarYen/Normalized-Attention-Guidance.git\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports + utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import hashlib\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# NAG pipelines (from the repo we installed)\n",
        "from nag import (\n",
        "    NAGFluxPipeline,\n",
        "    NAGFluxTransformer2DModel,\n",
        "    NAGStableDiffusion3Pipeline,\n",
        "    NAGStableDiffusionXLPipeline,\n",
        ")\n",
        "\n",
        "# SDXL helper deps (optional for some experiments)\n",
        "from diffusers import UNet2DConditionModel, LCMScheduler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def set_seed(seed: int = 0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def ensure_dir(p: str | Path) -> Path:\n",
        "    p = Path(p)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def save_image(img: Image.Image, path: str | Path):\n",
        "    path = Path(path)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    img.save(path)\n",
        "\n",
        "def sha1_of_file(path: str | Path, chunk_size: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha1()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f79c1514",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF token found in env: True\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)  # or set: HF_TOKEN=\"hf_...\"\n",
        "print(\"HF token found in env:\", bool(HF_TOKEN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Prompt set: COCO-5K loader (captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "COCO_ANN_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
        "COCO_IMG_URL = \"http://images.cocodataset.org/zips/val2014.zip\"\n",
        "\n",
        "DATA_DIR = ensure_dir(\"data\")\n",
        "COCO_DIR = ensure_dir(DATA_DIR / \"coco2014\")\n",
        "PROMPTS_PATH = COCO_DIR / \"coco5k_prompts.txt\"\n",
        "\n",
        "def _download(url: str, out_path: Path):\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if out_path.exists():\n",
        "        print(f\"Already exists: {out_path}\")\n",
        "        return\n",
        "    print(f\"Downloading: {url}\")\n",
        "    with requests.get(url, stream=True, timeout=60) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"content-length\", 0))\n",
        "        with open(out_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True) as pbar:\n",
        "            for chunk in r.iter_content(chunk_size=1<<20):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "def _unzip(zip_path: Path, dest: Path):\n",
        "    import zipfile\n",
        "    print(f\"Extracting {zip_path} -> {dest}\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(dest)\n",
        "\n",
        "def prepare_coco_annotations():\n",
        "    \"\"\"Downloads COCO 2014 annotations and extracts captions_val2014.json.\"\"\"\n",
        "    zip_path = COCO_DIR / \"annotations_trainval2014.zip\"\n",
        "    _download(COCO_ANN_URL, zip_path)\n",
        "    _unzip(zip_path, COCO_DIR)\n",
        "    ann_path = COCO_DIR / \"annotations\" / \"captions_val2014.json\"\n",
        "    if not ann_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing {ann_path}. Extraction may have failed.\")\n",
        "    return ann_path\n",
        "\n",
        "def make_coco5k_prompts(seed: int = 0, n: int = 5000) -> List[str]:\n",
        "    \"\"\"Creates a fixed prompt list from COCO val2014 captions.\"\"\"\n",
        "    ann_path = prepare_coco_annotations()\n",
        "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        d = json.load(f)\n",
        "    captions = [a[\"caption\"].strip() for a in d[\"annotations\"]]\n",
        "    # Paper uses COCO-5K; we sample deterministically.\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(captions)\n",
        "    prompts = captions[:n]\n",
        "    PROMPTS_PATH.write_text(\"\\\\n\".join(prompts), encoding=\"utf-8\")\n",
        "    print(f\"Wrote {len(prompts)} prompts -> {PROMPTS_PATH}\")\n",
        "    return prompts\n",
        "\n",
        "def load_prompts(path: Path = PROMPTS_PATH) -> List[str]:\n",
        "    if path.exists():\n",
        "        return [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
        "    print(\"Prompt file not found. Creating COCO-5K prompt list now...\")\n",
        "    return make_coco5k_prompts(seed=0, n=5000)\n",
        "\n",
        "# Smoke test: load first 5 prompts\n",
        "# prompts = load_prompts()[:5]\n",
        "# prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) (Optional) Reference images for FID/PFID (COCO val2014)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING: This is a large download (~6GB) and requires disk space.\n",
        "# If you don't need FID/PFID, you can skip this.\n",
        "\n",
        "VAL_IMG_DIR = COCO_DIR / \"val2014\"\n",
        "\n",
        "def prepare_coco_val_images():\n",
        "    zip_path = COCO_DIR / \"val2014.zip\"\n",
        "    _download(COCO_IMG_URL, zip_path)\n",
        "    _unzip(zip_path, COCO_DIR)\n",
        "    if not VAL_IMG_DIR.exists():\n",
        "        raise FileNotFoundError(f\"Expected {VAL_IMG_DIR} after extraction.\")\n",
        "    print(\"COCO val images ready:\", VAL_IMG_DIR)\n",
        "\n",
        "# Uncomment to download:\n",
        "# prepare_coco_val_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Experiment configs (baseline vs NAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'flux_schnell_4step': RunSpec(name='flux_schnell_4step', model_type='flux', model_id='black-forest-labs/FLUX.1-schnell', steps=4, width=1024, height=1024, guidance_scale=0.0, max_sequence_length=256, dtype=torch.bfloat16),\n",
              " 'sd3_5_turbo_8step': RunSpec(name='sd3_5_turbo_8step', model_type='sd3', model_id='stabilityai/stable-diffusion-3.5-large-turbo', steps=8, width=1024, height=1024, guidance_scale=0.0, max_sequence_length=256, dtype=torch.bfloat16),\n",
              " 'sdxl_base_25step': RunSpec(name='sdxl_base_25step', model_type='sdxl', model_id='stabilityai/stable-diffusion-xl-base-1.0', steps=25, width=1024, height=1024, guidance_scale=2.0, max_sequence_length=256, dtype=torch.bfloat16)}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Universal negative prompt used in the paper's quantitative comparison.\n",
        "UNIVERSAL_NEG_PROMPT = \"Low resolution, blurry\"\n",
        "\n",
        "# Paper Table-5 hyperparameters (ϕ = nag_scale, τ = nag_tau, α = nag_alpha).\n",
        "# You can adjust if you want to match the repo demos (they often use nag_scale=5 for Flux).\n",
        "PAPER_DEFAULTS = {\n",
        "    # Few-step DiT\n",
        "    \"flux.1-schnell\": dict(nag_scale=4.0, nag_tau=2.5, nag_alpha=0.25, steps=4, guidance_scale=0.0),\n",
        "    \"flux.1-dev\":     dict(nag_scale=4.0, nag_tau=2.5, nag_alpha=0.25, steps=25, guidance_scale=0.0),\n",
        "    # Multi-step UNet (SDXL)\n",
        "    \"sdxl\":           dict(nag_scale=2.0, nag_tau=2.5, nag_alpha=0.25, steps=25, guidance_scale=2.0),\n",
        "    # SD3.5 turbo (few-step-ish)\n",
        "    \"sd3.5-large-turbo\": dict(nag_scale=3.0, nag_tau=2.5, nag_alpha=0.25, steps=8, guidance_scale=0.0),\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class RunSpec:\n",
        "    name: str\n",
        "    model_type: str               # \"flux\", \"sdxl\", \"sd3\"\n",
        "    model_id: str\n",
        "    steps: int\n",
        "    width: int = 1024\n",
        "    height: int = 1024\n",
        "    guidance_scale: float = 0.0   # CFG; many distilled models use 0 or 1\n",
        "    max_sequence_length: int = 256\n",
        "    dtype: torch.dtype = torch.bfloat16\n",
        "\n",
        "# Suggested experiment set (edit as needed)\n",
        "RUNS: Dict[str, RunSpec] = {\n",
        "    \"flux_schnell_4step\": RunSpec(\n",
        "        name=\"flux_schnell_4step\",\n",
        "        model_type=\"flux\",\n",
        "        model_id=\"black-forest-labs/FLUX.1-schnell\",\n",
        "        steps=PAPER_DEFAULTS[\"flux.1-schnell\"][\"steps\"],\n",
        "        width=1024, height=1024,\n",
        "        guidance_scale=PAPER_DEFAULTS[\"flux.1-schnell\"][\"guidance_scale\"],\n",
        "        dtype=torch.bfloat16,\n",
        "    ),\n",
        "    \"sd3_5_turbo_8step\": RunSpec(\n",
        "        name=\"sd3_5_turbo_8step\",\n",
        "        model_type=\"sd3\",\n",
        "        model_id=\"stabilityai/stable-diffusion-3.5-large-turbo\",\n",
        "        steps=PAPER_DEFAULTS[\"sd3.5-large-turbo\"][\"steps\"],\n",
        "        width=1024, height=1024,\n",
        "        guidance_scale=PAPER_DEFAULTS[\"sd3.5-large-turbo\"][\"guidance_scale\"],\n",
        "        dtype=torch.bfloat16,\n",
        "    ),\n",
        "    # SDXL example (can be heavy). In the paper they test SDXL on multi-step and also DMD2 4-step.\n",
        "    # Below is a MULTI-STEP SDXL run on the base model; it may not exactly match paper's distilled checkpoints unless you load them.\n",
        "    \"sdxl_base_25step\": RunSpec(\n",
        "        name=\"sdxl_base_25step\",\n",
        "        model_type=\"sdxl\",\n",
        "        model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        steps=PAPER_DEFAULTS[\"sdxl\"][\"steps\"],\n",
        "        width=1024, height=1024,\n",
        "        guidance_scale=PAPER_DEFAULTS[\"sdxl\"][\"guidance_scale\"],\n",
        "        dtype=torch.bfloat16,\n",
        "    ),\n",
        "}\n",
        "\n",
        "RUNS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Load pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF token found in env: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import Any\n",
        "\n",
        "def load_pipeline(run: RunSpec, hf_token: Optional[str] = None):\n",
        "    \"\"\"Loads a pipeline for the given run spec.\"\"\"\n",
        "    if run.model_type == \"flux\":\n",
        "        transformer = NAGFluxTransformer2DModel.from_pretrained(\n",
        "            run.model_id,\n",
        "            subfolder=\"transformer\",\n",
        "            torch_dtype=run.dtype,\n",
        "            token=hf_token,\n",
        "        )\n",
        "        pipe = NAGFluxPipeline.from_pretrained(\n",
        "            run.model_id,\n",
        "            transformer=transformer,\n",
        "            torch_dtype=run.dtype,\n",
        "            token=hf_token,\n",
        "        )\n",
        "        pipe.to(\"cuda\")\n",
        "        return pipe\n",
        "\n",
        "    if run.model_type == \"sd3\":\n",
        "        pipe = NAGStableDiffusion3Pipeline.from_pretrained(\n",
        "            run.model_id,\n",
        "            torch_dtype=run.dtype,\n",
        "            token=hf_token,\n",
        "        )\n",
        "        pipe.to(\"cuda\")\n",
        "        return pipe\n",
        "\n",
        "    if run.model_type == \"sdxl\":\n",
        "        pipe = NAGStableDiffusionXLPipeline.from_pretrained(\n",
        "            run.model_id,\n",
        "            torch_dtype=run.dtype,\n",
        "            variant=\"fp16\" if run.dtype in (torch.float16, torch.bfloat16) else None,\n",
        "        ).to(\"cuda\")\n",
        "        return pipe\n",
        "\n",
        "    raise ValueError(f\"Unknown model_type: {run.model_type}\")\n",
        "\n",
        "# Put your HF token here if needed (Flux may be gated)\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)  # or set: HF_TOKEN=\"hf_...\"\n",
        "print(\"HF token found in env:\", bool(HF_TOKEN))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Image generation (baseline vs NAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@torch.inference_mode()\n",
        "def generate_batch(\n",
        "    pipe,\n",
        "    run: RunSpec,\n",
        "    prompts: List[str],\n",
        "    *,\n",
        "    out_dir: Path,\n",
        "    seed: int,\n",
        "    mode: str,  # \"baseline\" or \"nag\"\n",
        "    nag_negative_prompt: str = UNIVERSAL_NEG_PROMPT,\n",
        "    nag_scale: float = 4.0,\n",
        "    nag_tau: float = 2.5,\n",
        "    nag_alpha: float = 0.25,\n",
        "    batch_size: int = 1,\n",
        "):\n",
        "    \"\"\"Generate images and save them. Returns list of saved file paths.\"\"\"\n",
        "    assert mode in {\"baseline\", \"nag\"}\n",
        "    out_dir = ensure_dir(out_dir)\n",
        "\n",
        "    saved = []\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Most pipelines accept generator for determinism.\n",
        "    gen = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=f\"gen[{mode}]\"):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "        kwargs = dict(\n",
        "            prompt=batch_prompts,\n",
        "            num_inference_steps=run.steps,\n",
        "            guidance_scale=run.guidance_scale,\n",
        "            generator=gen,\n",
        "        )\n",
        "\n",
        "        # Some models (Flux) expose max_sequence_length\n",
        "        if \"max_sequence_length\" in pipe.__call__.__code__.co_varnames:\n",
        "            kwargs[\"max_sequence_length\"] = run.max_sequence_length\n",
        "\n",
        "        # Image size (some pipelines accept width/height; others might infer)\n",
        "        if \"width\" in pipe.__call__.__code__.co_varnames:\n",
        "            kwargs[\"width\"] = run.width\n",
        "        if \"height\" in pipe.__call__.__code__.co_varnames:\n",
        "            kwargs[\"height\"] = run.height\n",
        "\n",
        "        if mode == \"nag\":\n",
        "            kwargs.update(\n",
        "                nag_negative_prompt=nag_negative_prompt,\n",
        "                nag_scale=nag_scale,\n",
        "                nag_tau=nag_tau,\n",
        "                nag_alpha=nag_alpha,\n",
        "            )\n",
        "\n",
        "        out = pipe(**kwargs)\n",
        "        images = out.images if hasattr(out, \"images\") else out.frames[0]  # video pipelines not used here\n",
        "\n",
        "        for j, img in enumerate(images):\n",
        "            idx = i + j\n",
        "            fn = out_dir / f\"{idx:05d}.png\"\n",
        "            save_image(img, fn)\n",
        "            saved.append(fn)\n",
        "\n",
        "    # Save run metadata for reproducibility\n",
        "    meta = dict(\n",
        "        run=run.__dict__,\n",
        "        mode=mode,\n",
        "        seed=seed,\n",
        "        nag_negative_prompt=nag_negative_prompt if mode==\"nag\" else None,\n",
        "        nag_params=dict(nag_scale=nag_scale, nag_tau=nag_tau, nag_alpha=nag_alpha) if mode==\"nag\" else None,\n",
        "        n_images=len(saved),\n",
        "    )\n",
        "    (out_dir / \"run_meta.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "    return saved\n",
        "\n",
        "def get_nag_params_for_run(run: RunSpec):\n",
        "    if run.model_type == \"flux\":\n",
        "        d = PAPER_DEFAULTS[\"flux.1-schnell\"] if \"schnell\" in run.model_id.lower() else PAPER_DEFAULTS[\"flux.1-dev\"]\n",
        "    elif run.model_type == \"sd3\":\n",
        "        d = PAPER_DEFAULTS[\"sd3.5-large-turbo\"]\n",
        "    elif run.model_type == \"sdxl\":\n",
        "        d = PAPER_DEFAULTS[\"sdxl\"]\n",
        "    else:\n",
        "        raise ValueError(run.model_type)\n",
        "    return d[\"nag_scale\"], d[\"nag_tau\"], d[\"nag_alpha\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Quick smoke test (generate a few images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1286faf6c5ac407faa8387d7084d1048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5969c1a9e223474ea359a3f8a6ac3c3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5181e390efe64e2889f21db4ce82f44f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0590f7b29e6846719c92b22cc49c5aaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 3.06 MiB is free. Process 3978999 has 37.35 GiB memory in use. Process 2268732 has 668.00 MiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 1.04 GiB is allocated by PyTorch, and 14.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m test_prompts = load_prompts()[:\u001b[32m8\u001b[39m]\n\u001b[32m      7\u001b[39m seed = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m pipe = \u001b[43mload_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m nag_scale, nag_tau, nag_alpha = get_nag_params_for_run(run)\n\u001b[32m     13\u001b[39m OUT_ROOT = ensure_dir(\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m) / run.name\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_pipeline\u001b[39m\u001b[34m(run, hf_token)\u001b[39m\n\u001b[32m      6\u001b[39m     transformer = NAGFluxTransformer2DModel.from_pretrained(\n\u001b[32m      7\u001b[39m         run.model_id,\n\u001b[32m      8\u001b[39m         subfolder=\u001b[33m\"\u001b[39m\u001b[33mtransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         torch_dtype=run.dtype,\n\u001b[32m     10\u001b[39m         token=hf_token,\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m     pipe = NAGFluxPipeline.from_pretrained(\n\u001b[32m     13\u001b[39m         run.model_id,\n\u001b[32m     14\u001b[39m         transformer=transformer,\n\u001b[32m     15\u001b[39m         torch_dtype=run.dtype,\n\u001b[32m     16\u001b[39m         token=hf_token,\n\u001b[32m     17\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pipe\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run.model_type == \u001b[33m\"\u001b[39m\u001b[33msd3\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:545\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m     module.to(device=device)\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    548\u001b[39m     module.dtype == torch.float16\n\u001b[32m    549\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    550\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    552\u001b[39m ):\n\u001b[32m    553\u001b[39m     logger.warning(\n\u001b[32m    554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    558\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (4 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/okashurin/envs/nag/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 3.06 MiB is free. Process 3978999 has 37.35 GiB memory in use. Process 2268732 has 668.00 MiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 1.04 GiB is allocated by PyTorch, and 14.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "\n",
        "# Choose a run:\n",
        "run_key = \"flux_schnell_4step\"  # change to \"sd3_5_turbo_8step\" / \"sdxl_base_25step\"\n",
        "run = RUNS[run_key]\n",
        "\n",
        "# Small subset for smoke test\n",
        "test_prompts = load_prompts()[:8]\n",
        "seed = 0\n",
        "\n",
        "pipe = load_pipeline(run, hf_token=HF_TOKEN)\n",
        "\n",
        "nag_scale, nag_tau, nag_alpha = get_nag_params_for_run(run)\n",
        "\n",
        "OUT_ROOT = ensure_dir(\"outputs\") / run.name\n",
        "baseline_dir = OUT_ROOT / \"baseline\"\n",
        "nag_dir = OUT_ROOT / \"nag\"\n",
        "\n",
        "baseline_paths = generate_batch(\n",
        "    pipe, run, test_prompts,\n",
        "    out_dir=baseline_dir,\n",
        "    seed=seed,\n",
        "    mode=\"baseline\",\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "nag_paths = generate_batch(\n",
        "    pipe, run, test_prompts,\n",
        "    out_dir=nag_dir,\n",
        "    seed=seed,\n",
        "    mode=\"nag\",\n",
        "    nag_negative_prompt=UNIVERSAL_NEG_PROMPT,\n",
        "    nag_scale=nag_scale,\n",
        "    nag_tau=nag_tau,\n",
        "    nag_alpha=nag_alpha,\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "baseline_paths[:2], nag_paths[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Metrics: CLIP Score, FID, Patch-FID (PFID), ImageReward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "from torchmetrics.multimodal import CLIPScore\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "# --- CLIP Score ---\n",
        "def compute_clip_score(image_paths: List[Path], prompts: List[str], device=\"cuda\", clip_model=\"openai/clip-vit-base-patch32\"):\n",
        "    metric = CLIPScore(model_name_or_path=clip_model).to(device)\n",
        "    # torchmetrics expects uint8 images [0, 255] as a tensor Bx3xHxW\n",
        "    to_tensor = transforms.Compose([\n",
        "        transforms.ToTensor(),  # 0..1 float\n",
        "        transforms.Lambda(lambda x: (x * 255).to(torch.uint8)),\n",
        "    ])\n",
        "\n",
        "    scores = []\n",
        "    for p, txt in tqdm(list(zip(image_paths, prompts)), desc=\"CLIPScore\"):\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        img_t = to_tensor(img).unsqueeze(0).to(device)\n",
        "        s = metric(img_t, [txt]).item()\n",
        "        scores.append(s)\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "# --- FID ---\n",
        "def _iter_images_as_uint8_tensor(image_paths: List[Path], size=(299, 299)):\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: (x * 255).to(torch.uint8)),\n",
        "    ])\n",
        "    for p in image_paths:\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        yield tfm(img)\n",
        "\n",
        "def compute_fid(gen_paths: List[Path], real_paths: List[Path], device=\"cuda\"):\n",
        "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=True).to(device)\n",
        "\n",
        "    for t in tqdm(_iter_images_as_uint8_tensor(real_paths), total=len(real_paths), desc=\"FID(real)\"):\n",
        "        fid.update(t.unsqueeze(0).to(device), real=True)\n",
        "\n",
        "    for t in tqdm(_iter_images_as_uint8_tensor(gen_paths), total=len(gen_paths), desc=\"FID(gen)\"):\n",
        "        fid.update(t.unsqueeze(0).to(device), real=False)\n",
        "\n",
        "    return float(fid.compute().item())\n",
        "\n",
        "# --- Patch-FID (PFID) ---\n",
        "def _extract_patches(img: Image.Image, grid: int = 4) -> List[Image.Image]:\n",
        "    \"\"\"Split image into grid x grid patches.\"\"\"\n",
        "    w, h = img.size\n",
        "    pw, ph = w // grid, h // grid\n",
        "    patches = []\n",
        "    for gy in range(grid):\n",
        "        for gx in range(grid):\n",
        "            left, top = gx * pw, gy * ph\n",
        "            patches.append(img.crop((left, top, left + pw, top + ph)))\n",
        "    return patches\n",
        "\n",
        "def compute_patch_fid(gen_paths: List[Path], real_paths: List[Path], device=\"cuda\", grid: int = 4):\n",
        "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=True).to(device)\n",
        "\n",
        "    # update real patches\n",
        "    for p in tqdm(real_paths, desc=\"PFID(real)\", total=len(real_paths)):\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        for patch in _extract_patches(img, grid=grid):\n",
        "            t = transforms.ToTensor()(patch)\n",
        "            t = (t * 255).to(torch.uint8)\n",
        "            fid.update(t.unsqueeze(0).to(device), real=True)\n",
        "\n",
        "    # update gen patches\n",
        "    for p in tqdm(gen_paths, desc=\"PFID(gen)\", total=len(gen_paths)):\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        for patch in _extract_patches(img, grid=grid):\n",
        "            t = transforms.ToTensor()(patch)\n",
        "            t = (t * 255).to(torch.uint8)\n",
        "            fid.update(t.unsqueeze(0).to(device), real=False)\n",
        "\n",
        "    return float(fid.compute().item())\n",
        "\n",
        "# --- ImageReward ---\n",
        "def compute_imagereward(image_paths: List[Path], prompts: List[str], device=\"cuda\"):\n",
        "    import ImageReward as IR\n",
        "    model = IR.load(\"ImageReward-v1.0\", device=device)\n",
        "    scores = []\n",
        "    for p, txt in tqdm(list(zip(image_paths, prompts)), desc=\"ImageReward\"):\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        s = model.score(txt, img)\n",
        "        scores.append(float(s))\n",
        "    return float(np.mean(scores)), scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Run evaluation on your generated folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def list_images_sorted(folder: Path) -> List[Path]:\n",
        "    return sorted([p for p in folder.glob(\"*.png\")])\n",
        "\n",
        "# Use the smoke test output folders by default:\n",
        "gen_baseline = list_images_sorted(baseline_dir)\n",
        "gen_nag = list_images_sorted(nag_dir)\n",
        "\n",
        "eval_prompts = test_prompts  # must match image ordering\n",
        "\n",
        "print(\"n(baseline):\", len(gen_baseline), \"n(nag):\", len(gen_nag), \"n(prompts):\", len(eval_prompts))\n",
        "\n",
        "# ---- CLIP score ----\n",
        "clip_mean_base, _ = compute_clip_score(gen_baseline, eval_prompts)\n",
        "clip_mean_nag, _ = compute_clip_score(gen_nag, eval_prompts)\n",
        "\n",
        "# ---- ImageReward ----\n",
        "ir_mean_base, _ = compute_imagereward(gen_baseline, eval_prompts)\n",
        "ir_mean_nag, _ = compute_imagereward(gen_nag, eval_prompts)\n",
        "\n",
        "results = pd.DataFrame([\n",
        "    dict(run=run.name, mode=\"baseline\", clip_score=clip_mean_base, image_reward=ir_mean_base),\n",
        "    dict(run=run.name, mode=\"nag\",      clip_score=clip_mean_nag,  image_reward=ir_mean_nag),\n",
        "])\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) (Optional) FID / PFID evaluation (requires COCO real images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# This section requires COCO validation images downloaded in section 4.\n",
        "# We align on the *same number of images* as generated (e.g., 5k).\n",
        "# NOTE: COCO val2014 has 40k images; paper uses COCO-5K subset. You should define which 5k images you use.\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "def list_coco_real_images(val_dir: Path, n: int) -> List[Path]:\n",
        "    all_imgs = sorted(val_dir.glob(\"*.jpg\"))\n",
        "    if len(all_imgs) < n:\n",
        "        raise ValueError(f\"Not enough COCO images: {len(all_imgs)} < {n}\")\n",
        "    return all_imgs[:n]  # deterministic prefix; change if you want a different fixed subset\n",
        "\n",
        "if VAL_IMG_DIR.exists():\n",
        "    real_paths = list_coco_real_images(VAL_IMG_DIR, n=len(gen_baseline))\n",
        "\n",
        "    fid_base = compute_fid(gen_baseline, real_paths)\n",
        "    fid_nag = compute_fid(gen_nag, real_paths)\n",
        "\n",
        "    pfid_base = compute_patch_fid(gen_baseline, real_paths, grid=4)\n",
        "    pfid_nag = compute_patch_fid(gen_nag, real_paths, grid=4)\n",
        "\n",
        "    results_fid = pd.DataFrame([\n",
        "        dict(run=run.name, mode=\"baseline\", fid=fid_base, pfid=pfid_base),\n",
        "        dict(run=run.name, mode=\"nag\",      fid=fid_nag,  pfid=pfid_nag),\n",
        "    ])\n",
        "    results_fid\n",
        "else:\n",
        "    print(\"COCO val images not found. Run prepare_coco_val_images() first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Full experiment runner (COCO-5K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(\n",
        "    run: RunSpec,\n",
        "    *,\n",
        "    prompts: List[str],\n",
        "    out_root: Path,\n",
        "    seed: int = 0,\n",
        "    hf_token: Optional[str] = None,\n",
        "    batch_size: int = 1,\n",
        "    nag_negative_prompt: str = UNIVERSAL_NEG_PROMPT,\n",
        "):\n",
        "    out_root = ensure_dir(out_root)\n",
        "    pipe = load_pipeline(run, hf_token=hf_token)\n",
        "\n",
        "    nag_scale, nag_tau, nag_alpha = get_nag_params_for_run(run)\n",
        "\n",
        "    baseline_dir = out_root / \"baseline\"\n",
        "    nag_dir = out_root / \"nag\"\n",
        "\n",
        "    base_paths = generate_batch(\n",
        "        pipe, run, prompts,\n",
        "        out_dir=baseline_dir,\n",
        "        seed=seed,\n",
        "        mode=\"baseline\",\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    nag_paths = generate_batch(\n",
        "        pipe, run, prompts,\n",
        "        out_dir=nag_dir,\n",
        "        seed=seed,\n",
        "        mode=\"nag\",\n",
        "        nag_negative_prompt=nag_negative_prompt,\n",
        "        nag_scale=nag_scale,\n",
        "        nag_tau=nag_tau,\n",
        "        nag_alpha=nag_alpha,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    # Always compute CLIP + ImageReward (no ref needed)\n",
        "    clip_base, _ = compute_clip_score(base_paths, prompts)\n",
        "    clip_nag, _ = compute_clip_score(nag_paths, prompts)\n",
        "\n",
        "    ir_base, _ = compute_imagereward(base_paths, prompts)\n",
        "    ir_nag, _ = compute_imagereward(nag_paths, prompts)\n",
        "\n",
        "    metrics = pd.DataFrame([\n",
        "        dict(run=run.name, mode=\"baseline\", clip_score=clip_base, image_reward=ir_base),\n",
        "        dict(run=run.name, mode=\"nag\",      clip_score=clip_nag,  image_reward=ir_nag),\n",
        "    ])\n",
        "\n",
        "    # Optionally compute FID/PFID if real images exist\n",
        "    if VAL_IMG_DIR.exists():\n",
        "        real_paths = list_coco_real_images(VAL_IMG_DIR, n=len(prompts))\n",
        "        fid_base = compute_fid(base_paths, real_paths)\n",
        "        fid_nag = compute_fid(nag_paths, real_paths)\n",
        "        pfid_base = compute_patch_fid(base_paths, real_paths, grid=4)\n",
        "        pfid_nag = compute_patch_fid(nag_paths, real_paths, grid=4)\n",
        "        metrics.loc[metrics[\"mode\"]==\"baseline\", \"fid\"] = fid_base\n",
        "        metrics.loc[metrics[\"mode\"]==\"nag\", \"fid\"] = fid_nag\n",
        "        metrics.loc[metrics[\"mode\"]==\"baseline\", \"pfid\"] = pfid_base\n",
        "        metrics.loc[metrics[\"mode\"]==\"nag\", \"pfid\"] = pfid_nag\n",
        "\n",
        "    metrics.to_csv(out_root / \"metrics.csv\", index=False)\n",
        "    return metrics\n",
        "\n",
        "# Example: run on 128 prompts first, then scale up\n",
        "# prompts_5k = load_prompts()[:5000]\n",
        "# metrics = run_experiment(RUNS[\"flux_schnell_4step\"], prompts=prompts_5k, out_root=Path(\"outputs\")/RUNS[\"flux_schnell_4step\"].name, hf_token=HF_TOKEN, batch_size=1)\n",
        "# metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) SDXL distilled 4-step (DMD2) example (matches README)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# This section follows the repo README example for SDXL-DMD2 4-step.\n",
        "# It downloads a UNet checkpoint from 'tianweiy/DMD2' and uses NAGStableDiffusionXLPipeline.\n",
        "\n",
        "def load_sdxl_dmd2_4step(hf_token: Optional[str] = None, dtype=torch.bfloat16):\n",
        "    base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    repo_name = \"tianweiy/DMD2\"\n",
        "    ckpt_name = \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
        "\n",
        "    unet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(\"cuda\", dtype)\n",
        "    unet_path = hf_hub_download(repo_name, ckpt_name, token=hf_token)\n",
        "    unet.load_state_dict(torch.load(unet_path, map_location=\"cuda\"))\n",
        "\n",
        "    pipe = NAGStableDiffusionXLPipeline.from_pretrained(\n",
        "        base_model_id,\n",
        "        unet=unet,\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, original_inference_steps=4)\n",
        "    return pipe\n",
        "\n",
        "# Example usage:\n",
        "# pipe = load_sdxl_dmd2_4step(hf_token=HF_TOKEN)\n",
        "# img = pipe(\"A beautiful cyborg\", nag_negative_prompt=\"robot\", guidance_scale=0, nag_scale=3, num_inference_steps=4).images[0]\n",
        "# img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Tips for matching the paper more closely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Use the **same prompt list** and **same real-image subset** when reporting FID/PFID.  \n",
        "  In this notebook we create a deterministic prompt list (seeded shuffle) and select a deterministic prefix of COCO val images — but you should align these with the paper's exact protocol if you have it.\n",
        "\n",
        "- Use the **paper hyperparameters** (ϕ, τ, α) from Table-5:\n",
        "  - Flux: ϕ=4, τ=2.5, α=0.25  \n",
        "  - SDXL: ϕ=2, τ=2.5, α=0.25  \n",
        "  - SD3.5: ϕ=3, τ=2.5, α=0.25  \n",
        "\n",
        "- Many few-step models are designed for **guidance_scale=0 or 1**; CFG can degrade them.  \n",
        "  NAG is intended to restore negative prompting without requiring CFG.\n",
        "\n",
        "- For large-scale runs (COCO-5K), consider:\n",
        "  - `batch_size=1` for safety (VRAM)\n",
        "  - saving intermediate progress\n",
        "  - running on multiple GPUs / nodes with prompt sharding\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
